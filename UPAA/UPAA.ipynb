{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdHX5yj9LSur+waKtHVVeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IRevan/AI/blob/master/UPAA/UPAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptM9wnneF8_h",
        "outputId": "38f94bb0-0d2b-4ba4-dfe7-4c49ece06b2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 데이터 제너레이터 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = [line.strip().split('\\t') for line in file]\n",
        "        remove_set = [['']]\n",
        "        data = [i for i in data if i not in remove_set]\n",
        "        self.questions = [pair[0] for pair in data]\n",
        "        self.answers = [pair[0] for pair in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "\n",
        "# 전체 데이터에 대한 단어장 생성\n",
        "all_tokens = [word for sentence in CustomDataset(file_path).questions + CustomDataset(file_path).answers for word in sentence.split()]\n",
        "vocab = list(set(all_tokens))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# 모델 정의\n",
        "class SimpleChatbot(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleChatbot, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, lengths):\n",
        "        embedded = self.embedding(input)\n",
        "        packed = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.rnn(packed)\n",
        "        padded, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        output = self.fc(padded)\n",
        "        return output\n",
        "\n",
        "    def chat(self, initial_input, max_len=20):\n",
        "        # 초기 입력에 대한 처리\n",
        "        initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "        initial_embedded = self.embedding(initial_input_tensor)\n",
        "\n",
        "        # 초기 은닉 상태를 0으로 초기화\n",
        "        hidden = torch.zeros(1, 1, hidden_size).cuda()\n",
        "\n",
        "        # 디코더 실행\n",
        "        decoder_output, _ = self.rnn(initial_embedded, hidden)\n",
        "\n",
        "        # 예측 결과 초기화\n",
        "        predicted_sequence = [word_to_index['<sos>']]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output = self.fc(decoder_output.data)\n",
        "            _, top_index = output.topk(1)\n",
        "            predicted_sequence.append(top_index.item())\n",
        "\n",
        "            # 디코더 입력 업데이트\n",
        "            decoder_input = torch.tensor([top_index.item()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "            decoder_embedded = self.embedding(decoder_input)\n",
        "\n",
        "            # 다음 스텝 예측을 위해 다음 히든 상태 계산\n",
        "            decoder_output, _ = self.rnn(decoder_embedded, _)  # 이 부분을 수정\n",
        "\n",
        "            # <eos>를 만나면 종료\n",
        "            if top_index.item() == word_to_index['<eos>']:\n",
        "                break\n",
        "\n",
        "        # 예측된 시퀀스를 단어로 변환\n",
        "        predicted_words = [index_to_word[idx] for idx in predicted_sequence]\n",
        "\n",
        "        return predicted_words\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_size = len(vocab)  # 어휘 사전 크기\n",
        "hidden_size = 32\n",
        "output_size = len(vocab)  # 어휘 사전 크기\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "dataset = CustomDataset(file_path)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 초기화\n",
        "model = SimpleChatbot(input_size, hidden_size, output_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # 텍스트를 토큰 인덱스로 변환\n",
        "        questions_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in questions]\n",
        "        answers_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in answers]\n",
        "\n",
        "        # 패딩을 처리하기 위해 각 시퀀스의 길이를 구합니다.\n",
        "        questions_lengths = [len(seq) for seq in questions_tensor]\n",
        "        answers_lengths = [len(seq) for seq in answers_tensor]\n",
        "\n",
        "        # 패딩된 텐서 생성\n",
        "        questions_padded = nn.utils.rnn.pad_sequence(questions_tensor, batch_first=True)\n",
        "        answers_padded = nn.utils.rnn.pad_sequence(answers_tensor, batch_first=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(questions_padded, questions_lengths)\n",
        "        loss = criterion(output.view(-1, output_size), answers_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{epochs}, Loss: {total_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfw9GLn2n-K6",
        "outputId": "50968a7e-d724-41b4-b513-8f48ded542e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 3540.778185427189\n",
            "Epoch 100/1000, Loss: 0.00012093914685351592\n",
            "Epoch 200/1000, Loss: 4.793883514508934e-05\n",
            "Epoch 300/1000, Loss: 4.450732864924589e-05\n",
            "Epoch 400/1000, Loss: 4.203426154170131e-05\n",
            "Epoch 500/1000, Loss: 4.113318623666373e-05\n",
            "Epoch 600/1000, Loss: 4.104423642381505e-05\n",
            "Epoch 700/1000, Loss: 4.0920939154531766e-05\n",
            "Epoch 800/1000, Loss: 4.081914100240169e-05\n",
            "Epoch 900/1000, Loss: 4.0481658010627086e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 기능 테스트\n",
        "user_input = \"3D프린팅\"\n",
        "predicted_response = model.chat(user_input)\n",
        "print(f\"사용자: {user_input}\")\n",
        "print(f\"챗봇: {' '.join(predicted_response)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "ojswsgJfoEy2",
        "outputId": "a0b8e744-fa59-4a92-e57b-194ac4c25a0c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-fc81416f1119>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 챗봇 기능 테스트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3D프린팅\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredicted_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"사용자: {user_input}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"챗봇: {' '.join(predicted_response)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e746bfbdcfaa>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, initial_input, max_len)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# 예측 결과 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mpredicted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<sos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '<sos>'"
          ]
        }
      ]
    }
  ]
}