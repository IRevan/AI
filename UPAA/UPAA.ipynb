{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IRevan/AI/blob/master/UPAA/UPAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptM9wnneF8_h",
        "outputId": "b87dd945-2ac8-47ed-b3e4-d88e8b17188d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 데이터 제너레이터 정의\n",
        "class CustomDataset(Dataset):\n",
        "# CustomDataset 클래스의 __init__ 메서드에서 데이터 읽어오는 부분 수정\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = [line.strip().split('\\t') for line in file]\n",
        "        remove_set = [['']]\n",
        "        data = [i for i in data if i not in remove_set]\n",
        "        self.questions = ['<sos> ' + pair[0] + ' <eos>' for pair in data]  # '<sos>'와 '<eos>' 추가\n",
        "        self.answers = ['<sos> ' + pair[-1] + ' <eos>' for pair in data]  # '<sos>'와 '<eos>' 추가\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "\n",
        "# 특별 토큰을 포함한 어휘 리스트 생성\n",
        "all_tokens = [word for sentence in CustomDataset(file_path).questions + CustomDataset(file_path).answers for word in sentence.split()]\n",
        "vocab = list(set(all_tokens + ['<sos>', '<eos>']))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "class SimpleChatbot(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleChatbot, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, lengths):\n",
        "        embedded = self.embedding(input)\n",
        "        packed = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.rnn(packed)\n",
        "        padded, _ = pad_packed_sequence(output, batch_first=True, padding_value=0)\n",
        "        output = self.fc(padded)\n",
        "        return output\n",
        "\n",
        "    def chat(self, initial_input, max_len=20):\n",
        "        # 초기 입력에 대한 처리\n",
        "        initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "        initial_embedded = self.embedding(initial_input_tensor)\n",
        "\n",
        "        # 초기 은닉 상태를 0으로 초기화\n",
        "        hidden = torch.zeros(1, 1, hidden_size).cuda()\n",
        "\n",
        "        decoder_output, hidden = self.rnn(initial_embedded, hidden)\n",
        "\n",
        "        predicted_sequence = [word_to_index['<sos>']]\n",
        "\n",
        "        for _ in range(20):\n",
        "            output = self.fc(decoder_output)\n",
        "            _, top_index = output.topk(1)\n",
        "            predicted_sequence.append(top_index.item())\n",
        "\n",
        "            decoder_input = torch.tensor([top_index.item()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "            decoder_embedded = self.embedding(decoder_input)\n",
        "\n",
        "            decoder_output, hidden = self.rnn(decoder_embedded, hidden)\n",
        "\n",
        "            if top_index.item() == word_to_index['<eos>']:\n",
        "                break\n",
        "\n",
        "        # 예측된 시퀀스를 단어로 변환\n",
        "        predicted_words = [index_to_word[idx] for idx in predicted_sequence]\n",
        "\n",
        "        return predicted_words\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_size = len(vocab)  # 어휘 사전 크기\n",
        "hidden_size = 32\n",
        "output_size = len(vocab)  # 어휘 사전 크기\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "dataset = CustomDataset(file_path)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 초기화\n",
        "model = SimpleChatbot(input_size, hidden_size, output_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # 텍스트를 토큰 인덱스로 변환\n",
        "        questions_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in questions]\n",
        "        answers_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in answers]\n",
        "\n",
        "        # 패딩을 처리하기 위해 각 시퀀스의 길이를 구합니다.\n",
        "        questions_lengths = [len(seq) for seq in questions_tensor]\n",
        "        answers_lengths = [len(seq) for seq in answers_tensor]\n",
        "\n",
        "        # 패딩된 텐서 생성\n",
        "        questions_padded = nn.utils.rnn.pad_sequence(questions_tensor, batch_first=True)\n",
        "        answers_padded = nn.utils.rnn.pad_sequence(answers_tensor, batch_first=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(questions_padded, questions_lengths)\n",
        "        loss = criterion(output.view(-1, output_size), answers_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch}/{epochs}, Loss: {total_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD2XJL6OciZb",
        "outputId": "990d8499-3cf4-40ef-dfde-3d5211399146"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10, Loss: 2957.921025276184\n",
            "Epoch 1/10, Loss: 559.7160369455814\n",
            "Epoch 2/10, Loss: 226.8856780230999\n",
            "Epoch 3/10, Loss: 99.25282522290945\n",
            "Epoch 4/10, Loss: 35.21456088125706\n",
            "Epoch 5/10, Loss: 13.565986992791295\n",
            "Epoch 6/10, Loss: 6.35005354648456\n",
            "Epoch 7/10, Loss: 3.799939677119255\n",
            "Epoch 8/10, Loss: 2.6369836900848895\n",
            "Epoch 9/10, Loss: 1.9485031655058265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 기능 테스트\n",
        "user_input = \"3D프린팅융합기술센터\"\n",
        "model.eval()  # evaluation 모드로 설정\n",
        "\n",
        "# 초기 입력에 대한 처리\n",
        "initial_input_tensor = torch.tensor([word_to_index[word] for word in user_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "initial_embedded = model.embedding(initial_input_tensor)\n",
        "\n",
        "# 초기 은닉 상태를 0으로 초기화\n",
        "hidden = torch.zeros(1, 1, hidden_size).cuda()\n",
        "\n",
        "# 디코더 실행\n",
        "decoder_output, hidden = model.rnn(initial_embedded, hidden)\n",
        "\n",
        "# chat 메서드의 예측 결과 초기화 부분 수정\n",
        "predicted_sequence = [word_to_index['<sos>']]\n",
        "\n",
        "for _ in range(20):  # max_len은 20으로 설정\n",
        "    output = model.fc(decoder_output)\n",
        "    _, top_index = output.topk(1)\n",
        "    predicted_sequence.append(top_index.item())\n",
        "\n",
        "    # 디코더 입력 업데이트\n",
        "    decoder_input = torch.tensor([top_index.item()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "    decoder_embedded = model.embedding(decoder_input)\n",
        "\n",
        "    # 다음 스텝 예측을 위해 다음 히든 상태 계산\n",
        "    decoder_output, hidden = model.rnn(decoder_embedded, hidden)  # 수정된 부분\n",
        "\n",
        "    # <eos>를 만나면 종료\n",
        "    if top_index.item() == word_to_index['<eos>']:\n",
        "        break\n",
        "\n",
        "# 예측된 시퀀스를 단어로 변환\n",
        "predicted_words = [index_to_word[idx] for idx in predicted_sequence]\n",
        "\n",
        "print(f\"사용자: {user_input}\")\n",
        "print(f\"챗봇: {' '.join(predicted_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF_rf7-9cyBw",
        "outputId": "8b062517-70b5-4a78-ef03-b047005ea8ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자: 3D프린팅융합기술센터\n",
            "챗봇: <sos> 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터 3D프린팅융합기술센터\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import FastText\n",
        "import re\n",
        "\n",
        "# 한국어 FastText 모델 다운로드 경로\n",
        "korean_fasttext_model_path = '/content/drive/MyDrive/Colab Notebooks/fasttext/ko.bin'\n",
        "\n",
        "# 한국어 FastText 모델 로드\n",
        "korean_fasttext_model = FastText.load_fasttext_format(korean_fasttext_model_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수 문자 제거\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', text)\n",
        "    # 공백 기준으로 단어 분리\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "def embed_korean_text(text):\n",
        "    words = preprocess_text(text)\n",
        "    # 각 단어에 대한 임베딩 벡터 평균을 구함\n",
        "    vectors = [korean_fasttext_model.wv[word] for word in words if word in korean_fasttext_model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# .txt 파일에서 규정 텍스트 읽어오기 (한국어)\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    korean_regulation_text = file.read()\n",
        "\n",
        "# 규정 텍스트를 한국어로 임베딩\n",
        "korean_embedding = embed_korean_text(korean_regulation_text)\n",
        "\n",
        "# 데이터 제너레이터 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = [line.strip().split('\\t') for line in file]\n",
        "        remove_set = [['']]\n",
        "        data = [i for i in data if i not in remove_set]\n",
        "        self.questions = ['<sos> ' + pair[0] + ' <eos>' for pair in data]\n",
        "        self.answers = ['<sos> ' + pair[0] + ' <eos>' for pair in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "\n",
        "# 특별 토큰을 포함한 어휘 리스트 생성\n",
        "all_tokens = [word for sentence in CustomDataset(file_path).questions + CustomDataset(file_path).answers for word in sentence.split()]\n",
        "vocab = list(set(all_tokens + ['<sos>', '<eos>']))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for idx, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "wCyJ-30zZc5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cdfdb0-ceee-414b-a939-091801742af5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f7e74ec52fbb>:13: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
            "  korean_fasttext_model = FastText.load_fasttext_format(korean_fasttext_model_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleChatbot(nn.Module):\n",
        "    def __init__(self, embedding_model, hidden_size, output_size):\n",
        "        super(SimpleChatbot, self).__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "        self.rnn = nn.GRU(embedding_model.wv.vector_size, hidden_size, batch_first=True)  # Update input size here\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, lengths):\n",
        "    # Extract embeddings for each word separately and pad sequences\n",
        "        embedded = [torch.stack([torch.tensor(self.embedding_model.wv[word], dtype=torch.float32) for word in str(sentence).split()]) for sentence in input]\n",
        "\n",
        "    # Pad sequences\n",
        "        padded = nn.utils.rnn.pad_sequence(embedded, batch_first=True).cuda()\n",
        "\n",
        "    # Rest of the code remains the same\n",
        "        packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.rnn(packed)\n",
        "        padded, _ = pad_packed_sequence(output, batch_first=True, padding_value=0)\n",
        "        output = self.fc(padded)\n",
        "        return output\n",
        "\n",
        "    def chat(self, initial_input, max_len=20):\n",
        "        # 초기 입력에 대한 처리\n",
        "        initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split(\" \")], dtype=torch.long).unsqueeze(0).cuda()\n",
        "        initial_embedded = self.embedding_model.wv[initial_input].reshape(1, -1)\n",
        "\n",
        "        # 초기 은닉 상태를 0으로 초기화\n",
        "        hidden = torch.zeros(1, 1, hidden_size).cuda()\n",
        "\n",
        "        # 디코더 실행\n",
        "        decoder_output, hidden = self.rnn(initial_embedded, hidden)\n",
        "\n",
        "        # chat 메서드의 예측 결과 초기화 부분 수정\n",
        "        predicted_sequence = [word_to_index['<sos>']]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output = self.fc(decoder_output)\n",
        "            _, top_index = output.topk(1)\n",
        "            predicted_sequence.append(top_index.item())\n",
        "\n",
        "            # 디코더 입력 업데이트\n",
        "            decoder_input = torch.tensor([[top_index.item()]], dtype=torch.long).cuda()\n",
        "            decoder_embedded = self.embedding_model.wv[index_to_word[top_index.item()]].reshape(1, -1)\n",
        "\n",
        "            # 다음 스텝 예측을 위해 다음 히든 상태 계산\n",
        "            decoder_output, hidden = self.rnn(decoder_embedded, hidden)\n",
        "\n",
        "            # <eos>를 만나면 종료\n",
        "            if top_index.item() == word_to_index['<eos>']:\n",
        "                break\n",
        "\n",
        "        # 예측된 시퀀스를 단어로 변환\n",
        "        predicted_words = [index_to_word[idx] for idx in predicted_sequence]\n",
        "\n",
        "        return predicted_words\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_size = korean_fasttext_model.vector_size  # 임베딩 차원\n",
        "hidden_size = 32\n",
        "output_size = len(vocab)\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "dataset = CustomDataset(file_path)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 초기화\n",
        "model = SimpleChatbot(korean_fasttext_model, hidden_size, output_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # 텍스트를 토큰 인덱스로 변환\n",
        "        questions_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in questions]\n",
        "        answers_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in answers]\n",
        "\n",
        "        # 패딩을 처리하기 위해 각 시퀀스의 길이를 구합니다.\n",
        "        questions_lengths = [len(seq) for seq in questions_tensor]\n",
        "        answers_lengths = [len(seq) for seq in answers_tensor]\n",
        "\n",
        "        # 패딩된 텐서 생성\n",
        "        questions_padded = nn.utils.rnn.pad_sequence(questions_tensor, batch_first=True).cuda()\n",
        "        answers_padded = nn.utils.rnn.pad_sequence(answers_tensor, batch_first=True).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(questions_padded, questions_lengths)\n",
        "        loss = criterion(output.view(-1, output_size), answers_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch}/{epochs}, Loss: {total_loss}')\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "NbyWpa_aameO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff14eb3-ce59-426b-adcf-9d13b70d0cf8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2, Loss: 4214.158621072769\n",
            "Epoch 1/2, Loss: 2107.456081032753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleChatbot(\n",
              "  (rnn): GRU(200, 32, batch_first=True)\n",
              "  (fc): Linear(in_features=32, out_features=23847, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(chatbot_model, initial_input, max_len=20):\n",
        "    # Initialize the chatbot model for conversation\n",
        "    chatbot_model.eval()\n",
        "\n",
        "    # Convert the initial input to a PyTorch tensor\n",
        "    initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "\n",
        "    # Start the conversation\n",
        "    print(f\"User: {initial_input}\")\n",
        "\n",
        "    # Initialize the chat method to get the bot's response\n",
        "    response_words = chatbot_model.chat(initial_input_tensor, max_len)\n",
        "\n",
        "    # Convert the predicted sequence to words\n",
        "    response_words = [index_to_word[idx] for idx in response_words]\n",
        "\n",
        "    # Print the generated response\n",
        "    response = ' '.join(response_words[1:])  # Exclude the <sos> token\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "# Example usage:\n",
        "initial_input = \"장학금 수여\"\n",
        "chat_with_bot(model, initial_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "WI5ZjNTxYy_R",
        "outputId": "ec9dcd6b-0348-41b3-ee2c-987526d33ed9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: 장학금 수여\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c1c193ed71fb>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0minitial_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"장학금 수여\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mchat_with_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-c1c193ed71fb>\u001b[0m in \u001b[0;36mchat_with_bot\u001b[0;34m(chatbot_model, initial_input, max_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Initialize the chat method to get the bot's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresponse_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_input_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Convert the predicted sequence to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-13b3ecd4ea21>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, initial_input, max_len)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 초기 입력에 대한 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0minitial_input_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitial_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0minitial_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minitial_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_with_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str"
          ]
        }
      ]
    }
  ]
}