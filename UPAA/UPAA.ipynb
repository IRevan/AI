{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNP57k6+3auCehAP/ACdNAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IRevan/AI/blob/master/UPAA/UPAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptM9wnneF8_h",
        "outputId": "ceaabc3c-5eb1-4e40-ae12-d7ca157ad743"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import FastText\n",
        "import re\n",
        "\n",
        "# 한국어 FastText 모델 다운로드 경로\n",
        "korean_fasttext_model_path = '/content/drive/MyDrive/Colab Notebooks/fasttext/ko.bin'\n",
        "\n",
        "# 한국어 FastText 모델 로드\n",
        "korean_fasttext_model = FastText.load_fasttext_format(korean_fasttext_model_path)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수 문자 제거\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', text)\n",
        "    # 공백 기준으로 단어 분리\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "def embed_korean_text(text):\n",
        "    words = preprocess_text(text)\n",
        "    # 각 단어에 대한 임베딩 벡터 평균을 구함\n",
        "    vectors = [korean_fasttext_model.wv[word] for word in words if word in korean_fasttext_model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# .txt 파일에서 규정 텍스트 읽어오기 (한국어)\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    korean_regulation_text = file.read()\n",
        "\n",
        "# 규정 텍스트를 한국어로 임베딩\n",
        "korean_embedding = embed_korean_text(korean_regulation_text)\n",
        "\n",
        "# 데이터 제너레이터 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = [line.strip().split('\\t') for line in file]\n",
        "        remove_set = [['']]\n",
        "        data = [i for i in data if i not in remove_set]\n",
        "        self.questions = ['<sos> ' + pair[0] + ' <eos>' for pair in data]\n",
        "        self.answers = ['<sos> ' + pair[0] + ' <eos>' for pair in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.answers[idx]\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_output.txt'\n",
        "\n",
        "# 특별 토큰을 포함한 어휘 리스트 생성\n",
        "all_tokens = [word for sentence in CustomDataset(file_path).questions + CustomDataset(file_path).answers for word in sentence.split()]\n",
        "vocab = list(set(all_tokens + ['<sos>', '<eos>']))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for idx, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "wCyJ-30zZc5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleChatbot(nn.Module):\n",
        "    def __init__(self, embedding_model, hidden_size, output_size):\n",
        "        super(SimpleChatbot, self).__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "        self.rnn = nn.GRU(embedding_model.wv.vector_size, hidden_size, batch_first=True)  # Update input size here\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, lengths):\n",
        "    # Extract embeddings for each word separately and pad sequences\n",
        "        embedded = [torch.stack([torch.tensor(self.embedding_model.wv[word], dtype=torch.float32) for word in str(sentence).split()]) for sentence in input]\n",
        "\n",
        "    # Pad sequences\n",
        "        padded = nn.utils.rnn.pad_sequence(embedded, batch_first=True).cuda()\n",
        "\n",
        "    # Rest of the code remains the same\n",
        "        packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.rnn(packed)\n",
        "        padded, _ = pad_packed_sequence(output, batch_first=True, padding_value=0)\n",
        "        output = self.fc(padded)\n",
        "        return output\n",
        "\n",
        "    def chat(self, initial_input, max_len=20):\n",
        "        # 초기 입력에 대한 처리\n",
        "        initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "        initial_embedded = self.embedding_model.wv[initial_input].reshape(1, -1)\n",
        "\n",
        "        # 초기 은닉 상태를 0으로 초기화\n",
        "        hidden = torch.zeros(1, 1, hidden_size).cuda()\n",
        "\n",
        "        # 디코더 실행\n",
        "        decoder_output, hidden = self.rnn(initial_embedded, hidden)\n",
        "\n",
        "        # chat 메서드의 예측 결과 초기화 부분 수정\n",
        "        predicted_sequence = [word_to_index['<sos>']]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output = self.fc(decoder_output)\n",
        "            _, top_index = output.topk(1)\n",
        "            predicted_sequence.append(top_index.item())\n",
        "\n",
        "            # 디코더 입력 업데이트\n",
        "            decoder_input = torch.tensor([[top_index.item()]], dtype=torch.long).cuda()\n",
        "            decoder_embedded = self.embedding_model.wv[index_to_word[top_index.item()]].reshape(1, -1)\n",
        "\n",
        "            # 다음 스텝 예측을 위해 다음 히든 상태 계산\n",
        "            decoder_output, hidden = self.rnn(decoder_embedded, hidden)\n",
        "\n",
        "            # <eos>를 만나면 종료\n",
        "            if top_index.item() == word_to_index['<eos>']:\n",
        "                break\n",
        "\n",
        "        # 예측된 시퀀스를 단어로 변환\n",
        "        predicted_words = [index_to_word[idx] for idx in predicted_sequence]\n",
        "\n",
        "        return predicted_words\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_size = korean_fasttext_model.vector_size  # 임베딩 차원\n",
        "hidden_size = 32\n",
        "output_size = len(vocab)\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "dataset = CustomDataset(file_path)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 초기화\n",
        "model = SimpleChatbot(korean_fasttext_model, hidden_size, output_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        questions, answers = batch\n",
        "\n",
        "        # 텍스트를 토큰 인덱스로 변환\n",
        "        questions_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in questions]\n",
        "        answers_tensor = [torch.tensor([word_to_index[word] for word in sentence.split()], dtype=torch.long).cuda() for sentence in answers]\n",
        "\n",
        "        # 패딩을 처리하기 위해 각 시퀀스의 길이를 구합니다.\n",
        "        questions_lengths = [len(seq) for seq in questions_tensor]\n",
        "        answers_lengths = [len(seq) for seq in answers_tensor]\n",
        "\n",
        "        # 패딩된 텐서 생성\n",
        "        questions_padded = nn.utils.rnn.pad_sequence(questions_tensor, batch_first=True).cuda()\n",
        "        answers_padded = nn.utils.rnn.pad_sequence(answers_tensor, batch_first=True).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(questions_padded, questions_lengths)\n",
        "        loss = criterion(output.view(-1, output_size), answers_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{epochs}, Loss: {total_loss}')\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "NbyWpa_aameO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(chatbot_model, initial_input, max_len=20):\n",
        "    # Initialize the chatbot model for conversation\n",
        "    chatbot_model.eval()\n",
        "\n",
        "    # Convert the initial input to a PyTorch tensor\n",
        "    initial_input_tensor = torch.tensor([word_to_index[word] for word in initial_input.split()], dtype=torch.long).unsqueeze(0).cuda()\n",
        "\n",
        "    # Start the conversation\n",
        "    print(f\"User: {initial_input}\")\n",
        "\n",
        "    # Initialize the chat method to get the bot's response\n",
        "    response_words = chatbot_model.chat(initial_input_tensor, max_len)\n",
        "\n",
        "    # Convert the predicted sequence to words\n",
        "    response_words = [index_to_word[idx] for idx in response_words]\n",
        "\n",
        "    # Print the generated response\n",
        "    response = ' '.join(response_words[1:])  # Exclude the <sos> token\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "# Example usage:\n",
        "initial_input = \"장학금\"\n",
        "chat_with_bot(model, initial_input)"
      ],
      "metadata": {
        "id": "mcpccMsXg7Wf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}